# Sign-Language-Recognition-using-OpenCV
# âœ‹ Real-Time Hand Gesture Recognition System ğŸ¤–

## Project Overview ğŸ“š

This project aims to develop a robust real-time hand gesture recognition system using advanced computer vision and machine learning techniques. The primary objective is to accurately classify hand gestures corresponding to the letters of the alphabet, facilitating applications in sign language interpretation and assistive technologies for individuals with hearing impairments. ğŸŒŸ

## System Architecture ğŸ—ï¸

- **OpenCV**: Utilized for efficient video capture, ensuring real-time processing capabilities. ğŸ¥
- **MediaPipe**: Employed for its state-of-the-art hand tracking functionality, enabling precise detection and extraction of hand landmarks from video frames. ğŸ–ï¸
- **RandomForestClassifier**: Chosen for gesture classification due to its robustness and high accuracy in handling complex, multi-dimensional data. ğŸŒ²

## Project Phases ğŸ› ï¸

1. **Data Collection** ğŸ“¸
   - Set up the environment with essential libraries (OpenCV, MediaPipe, NumPy, Scikit-learn, Pickle).
   - Capture images of hand gestures using a webcam and store them in labeled folders.

2. **Data Preprocessing** ğŸ”„
   - Load captured images and extract hand landmarks using MediaPipe.
   - Normalize landmarks to ensure consistency in input features.
   - Combine normalized landmarks into feature vectors for model training.

3. **Model Training** ğŸ“
   - Split the dataset into training and testing sets.
   - Train a RandomForestClassifier and evaluate its performance.
   - Save the trained model using Pickle for future use.

4. **Real-Time Gesture Recognition** ğŸ•¹ï¸
   - Initialize MediaPipe and webcam for real-time hand tracking.
   - Process video frames to detect hand landmarks, predict gestures, and display results in real-time.

## Consolidation of Code ğŸ—‚ï¸

The project originally consisted of four separate code files, which have now been consolidated into a single Jupyter notebook (`project_notebook.ipynb`). The four different code snippets are:

1. **Data Collection Code**: Captures images of hand gestures using a webcam and saves them in labeled folders.
2. **Data Preprocessing Code**: Loads images, extracts hand landmarks using MediaPipe, normalizes the landmarks, and creates feature vectors.
3. **Model Training Code**: Splits the dataset, trains the RandomForestClassifier, evaluates its performance, and saves the model.
4. **Real-Time Gesture Recognition Code**: Initializes MediaPipe and webcam, processes video frames for hand landmark detection, predicts gestures, and displays results in real-time.

## Impact ğŸŒ

By achieving high accuracy in gesture recognition, this project demonstrates the feasibility of using machine learning models for dynamic gesture-based control systems. Potential applications include enhanced user interfaces, interactive gaming environments, and virtual communication tools, significantly improving accessibility for those with hearing impairments. ğŸ”Šâœ¨

---

ğŸš€ **Get started with this exciting project and contribute to making technology more accessible for everyone!**
